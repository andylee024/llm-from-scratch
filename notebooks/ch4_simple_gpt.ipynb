{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4 - Implement GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities - previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # build dataset\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self._build_dataset(text)\n",
    "\n",
    "    def _build_dataset(self, text):\n",
    "\n",
    "        context_length = self.max_length\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "        final_token_index = len(token_ids) - self.max_length\n",
    "\n",
    "        for i in range(0, final_token_index, self.stride):\n",
    "            input_chunk = token_ids[i: i+context_length]\n",
    "            target_chunk = token_ids[i+1 : i+1+context_length]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text=txt, tokenizer=tokenizer, max_length=max_length, stride=stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768, # Number of embedding dimensions\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of transformer layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False# Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "GPT2_LARGE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 1280, # Number of embedding dimensions\n",
    "    \"n_heads\": 20, # Number of attention heads\n",
    "    \"n_layers\": 36, # Number of transformer layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False# Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention w/ parallel matrix processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input dimensions\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = d_out // num_heads\n",
    "\n",
    "        # setup attention matrices\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # setup dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # setup linear\n",
    "        self.out_projection = nn.Linear(d_out, d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n, seq_length, _ = x.shape\n",
    "\n",
    "        # compute Q, K, V matrices\n",
    "        x_query = self.W_q(x)\n",
    "        x_key = self.W_k(x)\n",
    "        x_value = self.W_v(x)\n",
    "\n",
    "        # reshape to separate into Q = [Q1, Q2, ...], K = [K1, K2, ...]\n",
    "        x_query = x_query.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_query = x_query.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        x_key = x_key.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_key = x_key.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "        x_key = x_key.transpose(2, 3) # (n, num_heads, attention_dim, seq_length)\n",
    "\n",
    "        x_value = x_value.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_value = x_value.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        # compute attention scores (per-head)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "        mask_context = self.mask.bool()[:seq_length, :seq_length] \n",
    "        attention_scores = (x_query @ x_key)\n",
    "        attention_scores.masked_fill_(mask_context, -torch.inf)\n",
    "\n",
    "        # compute attention weights \n",
    "        # note : no dropout on scores (b/c dropout on -inf is not well-defined)\n",
    "        attention_weights = torch.softmax(attention_scores * dk_constant, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # compute context\n",
    "        context = attention_weights @ x_value\n",
    "\n",
    "        # reshape back to (n, seq_length, d_out)\n",
    "        context = context.contiguous().view(n, seq_length, self.d_out)\n",
    "        \n",
    "        # apply linear layer\n",
    "        return self.out_projection(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # read input X\n",
    "        _, seq_len = x.shape\n",
    "\n",
    "        # map X to embedding space\n",
    "        # embedding matrices take list of indices\n",
    "        tok_embeds = self.tok_emb(x)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # process embed(X) through architecture\n",
    "        # note : 12 transformer blocks is main processing\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) \n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    \"\"\"Activation function that's smoother than RELU\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    ")\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, x, max_new_tokens=100, context_size=10):\n",
    "\n",
    "    # iteratively generate new tokens (up to a limit)\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # get context for prediction\n",
    "        # note: equivalent to x[:, seq_length - context_size:]\n",
    "        x_conditioned = x[:, -context_size:] \n",
    "\n",
    "        # run model to predict next token\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_conditioned)\n",
    "\n",
    "        # decode next token prediction\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token_prediction = torch.argmax(next_token_probabilities, dim=-1, keepdim=True)\n",
    "\n",
    "        # create next context \n",
    "        x = torch.cat((x_conditioned, next_token_prediction), dim=1)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([2, 4])\n",
      "logits_shape : torch.Size([2, 4, 50257])\n",
      "Total parameters in the full model: 838220800\n",
      "Total parameters in the output head: 64328960\n",
      "Total parameters in the transformer block: 708249600\n",
      "Total parameters in a single attention layer: 6554880\n",
      "Total parameters in a single feed-forward layer: 13113600\n",
      "Total size of the model: 3197.56 MB\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# setup tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# setup data\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0) # (n x seq_length), each element is list of indices [idx1, idx2, ...]\n",
    "print(f\"batch shape: {batch.shape}\")\n",
    "\n",
    "# instantiate model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "gpt2_large_model = GPTModel(GPT2_LARGE_CONFIG)\n",
    "logits = model(batch)\n",
    "\n",
    "# inspect output\n",
    "print(f\"logits_shape : {logits.shape}\")\n",
    "\n",
    "# inspect parameters\n",
    "# exercise 4.1\n",
    "model = GPTModel(GPT2_LARGE_CONFIG)\n",
    "total_params_full = sum(p.numel() for p in model.parameters())\n",
    "total_params_out_head = sum(p.numel() for p in model.out_head.parameters())\n",
    "total_params_transformer_block = sum(p.numel() for p in model.trf_blocks.parameters())\n",
    "\n",
    "trf_block = model.trf_blocks[0]\n",
    "trf_attn = trf_block.att\n",
    "trf_ff = trf_block.ff\n",
    "total_params_single_attn = sum(p.numel() for p in trf_attn.parameters())\n",
    "total_params_single_ff = sum(p.numel() for p in trf_ff.parameters())\n",
    "\n",
    "# computer parameters\n",
    "print(f\"Total parameters in the full model: {total_params_full}\")\n",
    "print(f\"Total parameters in the output head: {total_params_out_head}\")\n",
    "print(f\"Total parameters in the transformer block: {total_params_transformer_block}\")\n",
    "print(f\"Total parameters in a single attention layer: {total_params_single_attn}\")\n",
    "print(f\"Total parameters in a single feed-forward layer: {total_params_single_ff}\")\n",
    "\n",
    "# compute memory requirements\n",
    "total_size_bytes = total_params_full * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 5 - Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_text : Every effort moves you wraps Booster iteratorakuyaChance Ib reluctant BjreeILL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# initialize start context\n",
    "start_context = \"Hello, I am\"\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "# setup\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "# prediction\n",
    "token_ids = generate_text_simple(model=model,\n",
    "                                 x=text_to_token_ids(start_context, tokenizer),\n",
    "                                 max_new_tokens=10,\n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(f\"output_text : {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"Calculates loss for a single batch given input & target\"\"\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"Iterates through data loader and calculates loss per batch\"\"\"\n",
    "    total_loss = 0.\n",
    "\n",
    "    # setup\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # iterate through batches , compute losses\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.019259028964573\n",
      "Validation loss: 11.028642654418945\n"
     ]
    }
   ],
   "source": [
    "# Example : 1 training loops\n",
    "\n",
    "# input data\n",
    "# file_path = \"/Users/andylee/Projects/llm-from-scratch/data/the-verdict.txt\"\n",
    "file_path = \"/datasets/the-verdict/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# split to train & validation batch\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "# create data loaders\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# setup devices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model.to(device)\n",
    "\n",
    "# run 1 single iteration of loss\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    \"\"\"Run a single forward pass of model to get predicted tokens\"\"\"\n",
    "\n",
    "    # set to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # encode X \n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "\n",
    "    # predict y\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, x=encoded, max_new_tokens=50, context_size=context_size)\n",
    "\n",
    "    # get text(y)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text)\n",
    "\n",
    "    # revert to train mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \"\"\"Training loop of NN\"\"\"\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # iterate through training batches \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # print losses based on eval_freq\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # generate new token predictions after each epoch \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.920, Val loss 10.086\n",
      "Every effort moves you,.                                                \n",
      "Ep 2 (Step 000010): Train loss 6.653, Val loss 7.034\n",
      "Every effort moves you, and, and the the of the the the the of the the of the the of the the of the the of the of the of the the of the the of the the the of the of the the of the of the of the of the\n",
      "Ep 3 (Step 000020): Train loss 5.048, Val loss 6.447\n",
      "Every effort moves you in the, and in the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
      "Ep 4 (Step 000030): Train loss 3.986, Val loss 6.302\n",
      "Every effort moves you, and I felt--as of the picture to the picture. \"--his, and I was not the fact, and I felt of the picture--as Jack's the donkey of the sketch of the donkey, and I felt to have of\n",
      "Ep 5 (Step 000040): Train loss 2.949, Val loss 6.232\n",
      "Every effort moves you know the was not that I was the was he was a little the last I had it was no he was the fact, the her poverty. \"--as Jack himself at my dear--the he was a little--his, I was his\n",
      "Ep 6 (Step 000050): Train loss 1.736, Val loss 6.238\n",
      "Every effort moves you?\" \"Oh, I felt as it's, and I had been I had been dead? I, in fact, in a little, I, I was a prod, as he was dead.\" \"I had been dead in the first\n",
      "Ep 7 (Step 000060): Train loss 0.946, Val loss 6.322\n",
      "Every effort moves you?\" \"Oh, I felt as it's, and I can in a self-century, I was one in an unusual degree to the Mrs. He placed them at my elbow and as I had been the man of the, in his\n",
      "Ep 8 (Step 000070): Train loss 0.404, Val loss 6.487\n",
      "Every effort moves you--his, and in the picture for the deep arm-chairs the last word. Gisburn, and the fullest reassurance. Gisburn, the his glory, he had it--the quality of Jack's \"There were days when I\n",
      "Ep 9 (Step 000080): Train loss 0.181, Val loss 6.522\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the same quality as his pictures--the quality of Jack's \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with herI must have let a rich's an awful simpleton, and in a degree he had the window-curtains, I saw that, when I felt, and in his\n",
      "Ep 11 (Step 000090): Train loss 0.064, Val loss 6.591\n",
      "Every effort moves you?\"  \"Yes--quite ins it to the irony. \"I could have let it's past!  He laughed again, and in delicate it. \"When he was dead.\"  \"I had and he was.\n",
      "Ep 12 (Step 000100): Train loss 0.034, Val loss 6.805\n",
      "Every effort moves you?\"  \"Yes--quite ins it to me to care a little: \"Yes--and by me!\"  Mrs. Gisburn drew back his glory, he had dropped his painting?\"      \" was.\n",
      "Ep 13 (Step 000110): Train loss 0.019, Val loss 6.851\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably and in a flash that he _'s an awful simpleton, you in a degree he had the window-curtains, I saw that, when I felt--that I found\n",
      "Ep 14 (Step 000120): Train loss 0.012, Val loss 6.900\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably and in a flash that he _'s an awful simpleton, you know, when, instinctively embarrassed by my the donkey again. I saw of Jack's \"There were days when I\n",
      "Ep 15 (Step 000130): Train loss 0.009, Val loss 6.937\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was dead.\"   \"Oh, stopping now\n",
      "Ep 16 (Step 000140): Train loss 0.007, Val loss 6.970\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was dead.\"   \"Oh, stopping now\n",
      "Ep 17 (Step 000150): Train loss 0.006, Val loss 6.968\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably it's \"There: \"Yes--the the fullest reassurance. It was just because she was _not_ interesting--if I saw that he was \"strongest,\" as his\n",
      "Ep 18 (Step 000160): Train loss 0.005, Val loss 7.000\n",
      "Every effort moves you?\"  \"Yes--quite ins it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture. And it.  \"Oh, stopping now\n",
      "Ep 19 (Step 000170): Train loss 0.004, Val loss 7.024\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 30\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=10, eval_iter=2,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-connection intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model w/out shortcut--\n",
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
      "\n",
      "\n",
      "--model w/ shortcut--\n",
      "layers.0.0.weight has gradient mean of 0.0014432291500270367\n",
      "layers.1.0.weight has gradient mean of 0.004846952389925718\n",
      "layers.2.0.weight has gradient mean of 0.004138893447816372\n",
      "layers.3.0.weight has gradient mean of 0.005915115587413311\n",
      "layers.4.0.weight has gradient mean of 0.032659437507390976\n"
     ]
    }
   ],
   "source": [
    "# Function to compute gradients in the model's backward pass\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)  # Forward pass\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target)  # Calculates loss based on how close the target and output are\n",
    "    loss.backward()  # Backward pass to calculate the gradients\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # run layer \n",
    "            layer_output = layer(x)\n",
    "\n",
    "            # if skip connection, then you sum the original (x) + layer-output and feed to next network\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "\n",
    "            # if no skip connection, then just layer_output = x, the input to next layer\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Model without shortcut\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "\n",
    "# Call the function to print gradients\n",
    "print(\"--model w/out shortcut--\")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "print(\"\\n\\n--model w/ shortcut--\")\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
