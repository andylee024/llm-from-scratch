{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4 - Implement GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768, # Number of embedding dimensions\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of transformer layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False# Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "GPT2_LARGE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 1280, # Number of embedding dimensions\n",
    "    \"n_heads\": 20, # Number of attention heads\n",
    "    \"n_layers\": 36, # Number of transformer layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False# Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention w/ parallel matrix processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input dimensions\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = d_out // num_heads\n",
    "\n",
    "        # setup attention matrices\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # setup dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # setup linear\n",
    "        self.out_projection = nn.Linear(d_out, d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n, seq_length, _ = x.shape\n",
    "\n",
    "        # compute Q, K, V matrices\n",
    "        x_query = self.W_q(x)\n",
    "        x_key = self.W_k(x)\n",
    "        x_value = self.W_v(x)\n",
    "\n",
    "        # reshape to separate into Q = [Q1, Q2, ...], K = [K1, K2, ...]\n",
    "        x_query = x_query.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_query = x_query.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        x_key = x_key.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_key = x_key.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "        x_key = x_key.transpose(2, 3) # (n, num_heads, attention_dim, seq_length)\n",
    "\n",
    "        x_value = x_value.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_value = x_value.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        # compute attention scores (per-head)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "        mask_context = self.mask.bool()[:seq_length, :seq_length] \n",
    "        attention_scores = (x_query @ x_key)\n",
    "        attention_scores.masked_fill_(mask_context, -torch.inf)\n",
    "\n",
    "        # compute attention weights \n",
    "        # note : no dropout on scores (b/c dropout on -inf is not well-defined)\n",
    "        attention_weights = torch.softmax(attention_scores * dk_constant, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # compute context\n",
    "        context = attention_weights @ x_value\n",
    "\n",
    "        # reshape back to (n, seq_length, d_out)\n",
    "        context = context.contiguous().view(n, seq_length, self.d_out)\n",
    "        \n",
    "        # apply linear layer\n",
    "        return self.out_projection(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # read input X\n",
    "        _, seq_len = x.shape\n",
    "\n",
    "        # map X to embedding space\n",
    "        # embedding matrices take list of indices\n",
    "        tok_embeds = self.tok_emb(x)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "\n",
    "        # process embed(X) through architecture\n",
    "        # note : 12 transformer blocks is main processing\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) \n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    \"\"\"Activation function that's smoother than RELU\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    ")\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([2, 4])\n",
      "logits_shape : torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# setup tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# setup data\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0) # (n x seq_length), each element is list of indices [idx1, idx2, ...]\n",
    "print(f\"batch shape: {batch.shape}\")\n",
    "\n",
    "# instantiate model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "gpt2_large_model = GPTModel(GPT2_LARGE_CONFIG)\n",
    "logits = model(batch)\n",
    "\n",
    "# inspect output\n",
    "print(f\"logits_shape : {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the full model: 838220800\n",
      "Total parameters in the output head: 64328960\n",
      "Total parameters in the transformer block: 708249600\n",
      "Total parameters in a single attention layer: 6554880\n",
      "Total parameters in a single feed-forward layer: 13113600\n",
      "Total size of the model: 3197.56 MB\n"
     ]
    }
   ],
   "source": [
    "model = gpt2_large_model\n",
    "# exercise 4.1\n",
    "total_params_full = sum(p.numel() for p in model.parameters())\n",
    "total_params_out_head = sum(p.numel() for p in model.out_head.parameters())\n",
    "total_params_transformer_block = sum(p.numel() for p in model.trf_blocks.parameters())\n",
    "\n",
    "trf_block = model.trf_blocks[0]\n",
    "trf_attn = trf_block.att\n",
    "trf_ff = trf_block.ff\n",
    "total_params_single_attn = sum(p.numel() for p in trf_attn.parameters())\n",
    "total_params_single_ff = sum(p.numel() for p in trf_ff.parameters())\n",
    "\n",
    "print(f\"Total parameters in the full model: {total_params_full}\")\n",
    "print(f\"Total parameters in the output head: {total_params_out_head}\")\n",
    "print(f\"Total parameters in the transformer block: {total_params_transformer_block}\")\n",
    "print(f\"Total parameters in a single attention layer: {total_params_single_attn}\")\n",
    "print(f\"Total parameters in a single feed-forward layer: {total_params_single_ff}\")\n",
    "\n",
    "# compute memory requirements of LLM\n",
    "total_size_bytes = total_params_full * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, x, max_new_tokens=100, context_size=10):\n",
    "\n",
    "    # iteratively generate new tokens (up to a limit)\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # get context for prediction\n",
    "        # note: equivalent to x[:, seq_length - context_size:]\n",
    "        x_conditioned = x[:, -context_size:] \n",
    "\n",
    "        # run model to predict next token\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_conditioned)\n",
    "\n",
    "        # decode next token prediction\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token_prediction = torch.argmax(next_token_probabilities, dim=-1, keepdim=True)\n",
    "\n",
    "        # create next context \n",
    "        x = torch.cat((x_conditioned, next_token_prediction), dim=1)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 49776, 30351, 35004, 45640, 39169, 13714]])\n",
      "Output length: 10\n",
      "Hello, I amobook async Oilersuphem Amendments productivity\n"
     ]
    }
   ],
   "source": [
    "# initialize start context\n",
    "start_context = \"Hello, I am\"\n",
    "\n",
    "# encode input\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "# run model\n",
    "model.eval()\n",
    "out = generate_text_simple(model=model,x=encoded_tensor,max_new_tokens=6,context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "# decode text\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-connection intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model w/out shortcut--\n",
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n",
      "\n",
      "\n",
      "--model w/ shortcut--\n",
      "layers.0.0.weight has gradient mean of 0.0014432319439947605\n",
      "layers.1.0.weight has gradient mean of 0.004846962168812752\n",
      "layers.2.0.weight has gradient mean of 0.0041389018297195435\n",
      "layers.3.0.weight has gradient mean of 0.00591512955725193\n",
      "layers.4.0.weight has gradient mean of 0.03265950828790665\n"
     ]
    }
   ],
   "source": [
    "# Function to compute gradients in the model's backward pass\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)  # Forward pass\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target)  # Calculates loss based on how close the target and output are\n",
    "    loss.backward()  # Backward pass to calculate the gradients\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # run layer \n",
    "            layer_output = layer(x)\n",
    "\n",
    "            # if skip connection, then you sum the original (x) + layer-output and feed to next network\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "\n",
    "            # if no skip connection, then just layer_output = x, the input to next layer\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Model without shortcut\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "\n",
    "# Call the function to print gradients\n",
    "print(\"--model w/out shortcut--\")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "print(\"\\n\\n--model w/ shortcut--\")\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
