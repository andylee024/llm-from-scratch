{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch\n",
    "This notebook contains code for LLM-from-scratch book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 3 - Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple attention example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- attention --\n",
      "token[1]: tensor([0.5500, 0.8700, 0.6600])\n",
      "A(.) is affinity\n",
      "w(0) = A(x(1), x(0)) : tensor([0.1455])\n",
      "w(1) = A(x(1), x(1)) : tensor([0.2278])\n",
      "w(2) = A(x(1), x(2)) : tensor([0.2249])\n",
      "w(3) = A(x(1), x(3)) : tensor([0.1285])\n",
      "w(4) = A(x(1), x(4)) : tensor([0.1077])\n",
      "w(5) = A(x(1), x(5)) : tensor([0.1656])\n",
      "\n",
      "\n",
      "-- context --\n",
      "list_context_vectors :  torch.Size([6, 3])\n",
      "z(0) = w(0)* x[0] : tensor([0.0625, 0.0218, 0.1295])\n",
      "z(1) = w(1)* x[1] : tensor([0.1253, 0.1982, 0.1504])\n",
      "z(2) = w(2)* x[2] : tensor([0.1282, 0.1911, 0.1439])\n",
      "z(3) = w(3)* x[3] : tensor([0.0283, 0.0745, 0.0424])\n",
      "z(4) = w(4)* x[4] : tensor([0.0830, 0.0269, 0.0108])\n",
      "z(5) = w(5)* x[5] : tensor([0.0083, 0.1325, 0.0911])\n",
      "\n",
      "context_wrt_query:  torch.Size([1, 3])\n",
      "tensor([[0.4355, 0.6451, 0.5680]])\n",
      "\n",
      "\n",
      "-- vectorize --\n",
      "context shape:  torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts (x^3)\n",
    "    [0.22, 0.58, 0.33], # with (x^4)\n",
    "    [0.77, 0.25, 0.10], # one (x^5)\n",
    "    [0.05, 0.80, 0.55] # step (x^6)\n",
    "])\n",
    "\n",
    "# simple affinity : dot-product (to measure similarity)\n",
    "def affinity(x, y):\n",
    "    \"\"\"Given 2 vectors, compute affinity\"\"\"\n",
    "    return torch.dot(x, y)\n",
    "\n",
    "# step 1 : calculate attention weights \n",
    "# idea : If query q : how much should each token of input X (i.e. x1, x2, ...) be weighed in importance \n",
    "# attention(query, x) for all x in input\n",
    "query_idx = 1\n",
    "query_token = X[query_idx]\n",
    "attention_weights = torch.tensor([affinity(x_i, query_token) for (_, x_i) in enumerate(X)])\n",
    "attention_weights = torch.tensor([a / attention_weights.sum() for a in attention_weights])\n",
    "attention_weights = attention_weights.view(-1, 1)\n",
    "\n",
    "print(\"\\n\\n-- attention --\")\n",
    "print(f\"token[{query_idx}]: {query_token}\")\n",
    "print(\"A(.) is affinity\")\n",
    "for idx, score in enumerate(attention_weights):\n",
    "    print(f\"w({idx}) = A(x({query_idx}), x({idx})) : {score}\")\n",
    "\n",
    "# step 2 : compute context vectors  \n",
    "# idea : Given query q and attention weights, create \"information context\" using weighted sum approach\n",
    "# idea : \"information context\" tells LLM how to make use of all the input tokens\n",
    "query = X[1]\n",
    "list_context_vectors = attention_weights * X\n",
    "context_vector = list_context_vectors.sum(dim=0, keepdim=True)\n",
    "print(\"\\n\\n-- context --\")\n",
    "print(\"list_context_vectors : \", list_context_vectors.shape)\n",
    "for idx, vec in enumerate(list_context_vectors):\n",
    "    print(f\"z({idx}) = w({idx})* x[{idx}] : {vec}\")\n",
    "\n",
    "print(\"\\ncontext_wrt_query: \", context_vector.shape)\n",
    "print(context_vector)\n",
    "\n",
    "# step 3 - vectorize \n",
    "print(\"\\n\\n-- vectorize --\")\n",
    "attention_scores = X @ X.T # compute attention pair-wise for each x_i, x_j pair using dot-product \n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) # row_i = attention weights w.r.t x_i\n",
    "context_matrix = attention_weights @ X # output (n, k) where each row i is attention_context for x_i\n",
    "print(\"context shape: \", context_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention \n",
    "Self-attention introduces 3 trainable parameters ($W_q$(query), $W_k$(key), $W_v$(value)) matrices ontop of attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 7])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "d_in = X.shape[1]\n",
    "d_out = 7\n",
    "x_2 = X[1]\n",
    "\n",
    "# define trainable parameters\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# step 1 : map X into query(x) and key(x)\n",
    "X_query = X @ W_query \n",
    "X_key = X @ W_key \n",
    "\n",
    "# step 2 : compute attention scores\n",
    "# note : a_ij = query(x_i) dot key(x_j)\n",
    "d_k = X_key.shape[-1]\n",
    "attention_scores = X_query @ X_key.T \n",
    "attention_weights = torch.softmax(attention_scores / d_k **0.5, dim=-1)\n",
    "\n",
    "# step 3 : compute context \n",
    "# idea : context = attention_score * value \n",
    "X_value = X @ W_value\n",
    "context = attention_weights @ X_value\n",
    "\n",
    "# attention_weights.sum(dim=1, keepdim=True)\n",
    "print(X.shape)\n",
    "print(attention_scores.shape)\n",
    "print(attention_weights.shape)\n",
    "print(X_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def update_matrices(self, W_q, W_k, W_v):\n",
    "        self.W_query = nn.Parameter(W_q)\n",
    "        self.W_key = nn.Parameter(W_k)\n",
    "        self.W_value = nn.Parameter(W_v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = x @ self.W_query\n",
    "        x_key = x @ self.W_key\n",
    "        x_value = x @ self.W_value\n",
    "        dk_constant = x_key.shape[-1]\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores / dk_constant **0.5, dim=-1)\n",
    "\n",
    "        # step 3 : compute context \n",
    "        return attention_weights @ x_value\n",
    "\n",
    "\n",
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def _forward_full_attention(self, x):\n",
    "        \"\"\"Forward method with full attention\"\"\"\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores * dk_constant, dim=-1)\n",
    "\n",
    "        # step 3 : compute context\n",
    "        return attention_weights @ x_value\n",
    "\n",
    "    def _forward_masked_attention(self, x):\n",
    "\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "\n",
    "        # step 3 : compute attention w/ mask \n",
    "        # idea : cannot see future tokens, AI can only see past tokens for predictions\n",
    "        context_length = attention_scores.shape[0]\n",
    "        mask = torch.tril(torch.ones(context_length, context_length))\n",
    "        attention_scores_mask = attention_scores.masked_fill(~mask.bool(), -torch.inf)\n",
    "        attention_weights_mask = torch.softmax(attention_scores_mask * dk_constant, dim=-1)\n",
    "\n",
    "        # step 4 : compute context\n",
    "        return attention_weights_mask @ x_value\n",
    "\n",
    "    def _forward_masked_attention_v2(self, x):\n",
    "        batch, num_tokens, vocab_dim = x.shape\n",
    "\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "\n",
    "        print(f\"x_q: {x_query.shape}\")\n",
    "        print(f\"x_k: {x_key.shape}\")\n",
    "\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "        mask_context = self.mask.bool()[:num_tokens, :num_tokens] # each batch might have different length context\n",
    "\n",
    "        attn_scores = x_query @ x_key.transpose(1, 2) # 0 is batch so we keep that constant\n",
    "        print(f\"single_head_attn: {attn_scores.shape}\")\n",
    "        attn_scores.masked_fill_(mask_context, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores * dk_constant, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        return attn_weights @ x_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_masked_attention_v2(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention\"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualAttention(d_in=d_in, d_out=d_out, context_length=context_length, dropout=dropout, qkv_bias=qkv_bias)\n",
    "             for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention w/ parallel matrix processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input dimensions\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = d_out // num_heads\n",
    "\n",
    "        # setup attention matrices\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # setup dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n, seq_length, x_dim_in = x.shape\n",
    "\n",
    "        # compute Q, K, V matrices\n",
    "        x_query = self.W_q(x)\n",
    "        x_key = self.W_k(x)\n",
    "        x_value = self.W_v(x)\n",
    "\n",
    "        # reshape to separate into Q = [Q1, Q2, ...], K = [K1, K2, ...]\n",
    "        x_query = x_query.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_query = x_query.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        x_key = x_key.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_key = x_key.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "        x_key = x_key.transpose(2, 3) # (n, num_heads, attention_dim, seq_length)\n",
    "\n",
    "        # compute attention weights\n",
    "        attention_scores = x_query @ x_key \n",
    "        print(f\"multi_attention shape: {attention_scores.shape}\")\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_q: torch.Size([2, 6, 240])\n",
      "x_k: torch.Size([2, 6, 240])\n",
      "single_head_attn: torch.Size([2, 6, 6])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadAttention' object has no attribute 'head_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# multi_attention = MultiHeadAttentionWrapper(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout, num_heads=4)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m multi_attention \u001b[38;5;241m=\u001b[39m MultiHeadAttention(d_in\u001b[38;5;241m=\u001b[39mx_dim_in, d_out\u001b[38;5;241m=\u001b[39mx_dim_out, context_length\u001b[38;5;241m=\u001b[39mcontext_length, dropout\u001b[38;5;241m=\u001b[39mdropout, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m multi_attention(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 34\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m x_query \u001b[38;5;241m=\u001b[39m x_query\u001b[38;5;241m.\u001b[39mview(n, seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dim)\n\u001b[1;32m     32\u001b[0m x_query \u001b[38;5;241m=\u001b[39m x_query\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (n, num_heads, seq_length, attention_dim)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m x_key \u001b[38;5;241m=\u001b[39m x_key\u001b[38;5;241m.\u001b[39mview(n, seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     35\u001b[0m x_key \u001b[38;5;241m=\u001b[39m x_key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (n, num_heads, seq_length, attention_dim)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x_key \u001b[38;5;241m=\u001b[39m x_key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# (n, num_heads, attention_dim, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadAttention' object has no attribute 'head_dim'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "# input\n",
    "x = torch.stack([X, X], dim=0)\n",
    "n, seq_length, token_dim = x.shape\n",
    "\n",
    "# hyperparameters\n",
    "x_dim_in = token_dim\n",
    "x_dim_out = 240\n",
    "context_length = seq_length\n",
    "dropout = 0.1\n",
    "\n",
    "# create self attention\n",
    "single_attention = CasualAttention(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout)\n",
    "single_attention(x)\n",
    "# multi_attention = MultiHeadAttentionWrapper(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout, num_heads=4)\n",
    "multi_attention = MultiHeadAttention(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout, num_heads=1)\n",
    "multi_attention(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : torch.Size([1, 2, 3, 4])\n",
      "A.T : torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn((1, 2, 3, 4))\n",
    "B = torch.randn((2, 6, 8))\n",
    "\n",
    "print(f\"A : {A.shape}\")\n",
    "print(f\"A.T : {A.transpose(1, 2).shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
