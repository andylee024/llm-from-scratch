{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch\n",
    "This notebook contains code for LLM-from-scratch book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 3 - Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- attention --\n",
      "token[1]: tensor([0.5500, 0.8700, 0.6600])\n",
      "A(.) is affinity\n",
      "w(0) = A(x(1), x(0)) : tensor([0.1455])\n",
      "w(1) = A(x(1), x(1)) : tensor([0.2278])\n",
      "w(2) = A(x(1), x(2)) : tensor([0.2249])\n",
      "w(3) = A(x(1), x(3)) : tensor([0.1285])\n",
      "w(4) = A(x(1), x(4)) : tensor([0.1077])\n",
      "w(5) = A(x(1), x(5)) : tensor([0.1656])\n",
      "\n",
      "\n",
      "-- context --\n",
      "list_context_vectors :  torch.Size([6, 3])\n",
      "z(0) = w(0)* x[0] : tensor([0.0625, 0.0218, 0.1295])\n",
      "z(1) = w(1)* x[1] : tensor([0.1253, 0.1982, 0.1504])\n",
      "z(2) = w(2)* x[2] : tensor([0.1282, 0.1911, 0.1439])\n",
      "z(3) = w(3)* x[3] : tensor([0.0283, 0.0745, 0.0424])\n",
      "z(4) = w(4)* x[4] : tensor([0.0830, 0.0269, 0.0108])\n",
      "z(5) = w(5)* x[5] : tensor([0.0083, 0.1325, 0.0911])\n",
      "\n",
      "context_wrt_query:  torch.Size([1, 3])\n",
      "tensor([[0.4355, 0.6451, 0.5680]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts (x^3)\n",
    "    [0.22, 0.58, 0.33], # with (x^4)\n",
    "    [0.77, 0.25, 0.10], # one (x^5)\n",
    "    [0.05, 0.80, 0.55] # step (x^6)\n",
    "])\n",
    "\n",
    "# simple affinity : dot-product (to measure similarity)\n",
    "def affinity(x, y):\n",
    "    \"\"\"Given 2 vectors, compute affinity\"\"\"\n",
    "    return torch.dot(x, y)\n",
    "\n",
    "# step 1 : calculate attention weights \n",
    "# idea : If query q : how much should each token of input X (i.e. x1, x2, ...) be weighed in importance \n",
    "# attention(query, x) for all x in input\n",
    "query_idx = 1\n",
    "query_token = X[query_idx]\n",
    "attention_weights = torch.tensor([affinity(x_i, query_token) for (_, x_i) in enumerate(X)])\n",
    "attention_weights = torch.tensor([a / attention_weights.sum() for a in attention_weights])\n",
    "attention_weights = attention_weights.view(-1, 1)\n",
    "\n",
    "print(\"\\n\\n-- attention --\")\n",
    "print(f\"token[{query_idx}]: {query_token}\")\n",
    "print(\"A(.) is affinity\")\n",
    "for idx, score in enumerate(attention_weights):\n",
    "    print(f\"w({idx}) = A(x({query_idx}), x({idx})) : {score}\")\n",
    "\n",
    "# step 2 : compute context vectors  \n",
    "# idea : Given query q and attention weights, create \"information context\" using weighted sum approach\n",
    "# idea : \"information context\" tells LLM how to make use of all the input tokens\n",
    "query = X[1]\n",
    "list_context_vectors = attention_weights * X\n",
    "context_vector = list_context_vectors.sum(dim=0, keepdim=True)\n",
    "print(\"\\n\\n-- context --\")\n",
    "print(\"list_context_vectors : \", list_context_vectors.shape)\n",
    "for idx, vec in enumerate(list_context_vectors):\n",
    "    print(f\"z({idx}) = w({idx})* x[{idx}] : {vec}\")\n",
    "\n",
    "print(\"\\ncontext_wrt_query: \", context_vector.shape)\n",
    "print(context_vector)\n",
    "\n",
    "# step 3 - vectorize \n",
    "print(\"\\n\\n-- vectorize --\")\n",
    "attention_scores = X @ X.T # compute attention pair-wise for each x_i, x_j pair using dot-product \n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) # row_i = attention weights w.r.t x_i\n",
    "context_matrix = attention_weights @ X # output (n, k) where each row i is attention_context for x_i\n",
    "print(\"context shape: \", context_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- attention scores and weights shapes --\n",
      "attention_scores shape:  torch.Size([6, 6])\n",
      "attention_weights shape:  torch.Size([6, 6])\n",
      "context shape:  torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# vectorize attention\n",
    "attention_scores = X @ X.T\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "context_matrix = attention_weights @ X\n",
    "print(\"\\n\\n-- attention scores and weights shapes --\")\n",
    "print(\"attention_scores shape: \", attention_scores.shape)\n",
    "print(\"attention_weights shape: \", attention_weights.shape)\n",
    "print(\"context shape: \", context_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
