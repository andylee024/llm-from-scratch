{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch\n",
    "This notebook contains code for LLM-from-scratch book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 3 - Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple attention example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- attention --\n",
      "token[1]: tensor([0.5500, 0.8700, 0.6600])\n",
      "A(.) is affinity\n",
      "w(0) = A(x(1), x(0)) : tensor([0.1455])\n",
      "w(1) = A(x(1), x(1)) : tensor([0.2278])\n",
      "w(2) = A(x(1), x(2)) : tensor([0.2249])\n",
      "w(3) = A(x(1), x(3)) : tensor([0.1285])\n",
      "w(4) = A(x(1), x(4)) : tensor([0.1077])\n",
      "w(5) = A(x(1), x(5)) : tensor([0.1656])\n",
      "\n",
      "\n",
      "-- context --\n",
      "list_context_vectors :  torch.Size([6, 3])\n",
      "z(0) = w(0)* x[0] : tensor([0.0625, 0.0218, 0.1295])\n",
      "z(1) = w(1)* x[1] : tensor([0.1253, 0.1982, 0.1504])\n",
      "z(2) = w(2)* x[2] : tensor([0.1282, 0.1911, 0.1439])\n",
      "z(3) = w(3)* x[3] : tensor([0.0283, 0.0745, 0.0424])\n",
      "z(4) = w(4)* x[4] : tensor([0.0830, 0.0269, 0.0108])\n",
      "z(5) = w(5)* x[5] : tensor([0.0083, 0.1325, 0.0911])\n",
      "\n",
      "context_wrt_query:  torch.Size([1, 3])\n",
      "tensor([[0.4355, 0.6451, 0.5680]])\n",
      "\n",
      "\n",
      "-- vectorize --\n",
      "context shape:  torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts (x^3)\n",
    "    [0.22, 0.58, 0.33], # with (x^4)\n",
    "    [0.77, 0.25, 0.10], # one (x^5)\n",
    "    [0.05, 0.80, 0.55] # step (x^6)\n",
    "])\n",
    "\n",
    "# simple affinity : dot-product (to measure similarity)\n",
    "def affinity(x, y):\n",
    "    \"\"\"Given 2 vectors, compute affinity\"\"\"\n",
    "    return torch.dot(x, y)\n",
    "\n",
    "# step 1 : calculate attention weights \n",
    "# idea : If query q : how much should each token of input X (i.e. x1, x2, ...) be weighed in importance \n",
    "# attention(query, x) for all x in input\n",
    "query_idx = 1\n",
    "query_token = X[query_idx]\n",
    "attention_weights = torch.tensor([affinity(x_i, query_token) for (_, x_i) in enumerate(X)])\n",
    "attention_weights = torch.tensor([a / attention_weights.sum() for a in attention_weights])\n",
    "attention_weights = attention_weights.view(-1, 1)\n",
    "\n",
    "print(\"\\n\\n-- attention --\")\n",
    "print(f\"token[{query_idx}]: {query_token}\")\n",
    "print(\"A(.) is affinity\")\n",
    "for idx, score in enumerate(attention_weights):\n",
    "    print(f\"w({idx}) = A(x({query_idx}), x({idx})) : {score}\")\n",
    "\n",
    "# step 2 : compute context vectors  \n",
    "# idea : Given query q and attention weights, create \"information context\" using weighted sum approach\n",
    "# idea : \"information context\" tells LLM how to make use of all the input tokens\n",
    "query = X[1]\n",
    "list_context_vectors = attention_weights * X\n",
    "context_vector = list_context_vectors.sum(dim=0, keepdim=True)\n",
    "print(\"\\n\\n-- context --\")\n",
    "print(\"list_context_vectors : \", list_context_vectors.shape)\n",
    "for idx, vec in enumerate(list_context_vectors):\n",
    "    print(f\"z({idx}) = w({idx})* x[{idx}] : {vec}\")\n",
    "\n",
    "print(\"\\ncontext_wrt_query: \", context_vector.shape)\n",
    "print(context_vector)\n",
    "\n",
    "# step 3 - vectorize \n",
    "print(\"\\n\\n-- vectorize --\")\n",
    "attention_scores = X @ X.T # compute attention pair-wise for each x_i, x_j pair using dot-product \n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) # row_i = attention weights w.r.t x_i\n",
    "context_matrix = attention_weights @ X # output (n, k) where each row i is attention_context for x_i\n",
    "print(\"context shape: \", context_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention \n",
    "Self-attention introduces 3 trainable parameters ($W_q$(query), $W_k$(key), $W_v$(value)) matrices ontop of attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 7])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "d_in = X.shape[1]\n",
    "d_out = 7\n",
    "x_2 = X[1]\n",
    "\n",
    "# define trainable parameters\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# step 1 : map X into query(x) and key(x)\n",
    "X_query = X @ W_query \n",
    "X_key = X @ W_key \n",
    "\n",
    "# step 2 : compute attention scores\n",
    "# note : a_ij = query(x_i) dot key(x_j)\n",
    "d_k = X_key.shape[-1]\n",
    "attention_scores = X_query @ X_key.T \n",
    "attention_weights = torch.softmax(attention_scores / d_k **0.5, dim=-1)\n",
    "\n",
    "# step 3 : compute context \n",
    "# idea : context = attention_score * value \n",
    "X_value = X @ W_value\n",
    "context = attention_weights @ X_value\n",
    "\n",
    "# attention_weights.sum(dim=1, keepdim=True)\n",
    "print(X.shape)\n",
    "print(attention_scores.shape)\n",
    "print(attention_weights.shape)\n",
    "print(X_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def update_matrices(self, W_q, W_k, W_v):\n",
    "        self.W_query = nn.Parameter(W_q)\n",
    "        self.W_key = nn.Parameter(W_k)\n",
    "        self.W_value = nn.Parameter(W_v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = x @ self.W_query\n",
    "        x_key = x @ self.W_key\n",
    "        x_value = x @ self.W_value\n",
    "        dk_constant = x_key.shape[-1]\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores / dk_constant **0.5, dim=-1)\n",
    "\n",
    "        # step 3 : compute context \n",
    "        return attention_weights @ x_value\n",
    "\n",
    "\n",
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def _forward_full_attention(self, x):\n",
    "        \"\"\"Forward method with full attention\"\"\"\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores * dk_constant, dim=-1)\n",
    "\n",
    "        # step 3 : compute context\n",
    "        return attention_weights @ x_value\n",
    "\n",
    "    def _forward_masked_attention(self, x):\n",
    "\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "\n",
    "        # step 3 : compute attention w/ mask \n",
    "        # idea : cannot see future tokens, AI can only see past tokens for predictions\n",
    "        context_length = attention_scores.shape[0]\n",
    "        mask = torch.tril(torch.ones(context_length, context_length))\n",
    "        attention_scores_mask = attention_scores.masked_fill(~mask.bool(), -torch.inf)\n",
    "        attention_weights_mask = torch.softmax(attention_scores_mask * dk_constant, dim=-1)\n",
    "\n",
    "        # step 4 : compute context\n",
    "        return attention_weights_mask @ x_value\n",
    "\n",
    "    def _forward_masked_attention_v2(self, x):\n",
    "        batch, num_tokens, vocab_dim = x.shape\n",
    "\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "\n",
    "        print(f\"x_q: {x_query.shape}\")\n",
    "        print(f\"x_k: {x_key.shape}\")\n",
    "\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "        mask_context = self.mask.bool()[:num_tokens, :num_tokens] # each batch might have different length context\n",
    "\n",
    "        attn_scores = x_query @ x_key.transpose(1, 2) # 0 is batch so we keep that constant\n",
    "        print(f\"single_head_attn: {attn_scores.shape}\")\n",
    "        attn_scores.masked_fill_(mask_context, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores * dk_constant, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        return attn_weights @ x_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_masked_attention_v2(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention\"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualAttention(d_in=d_in, d_out=d_out, context_length=context_length, dropout=dropout, qkv_bias=qkv_bias)\n",
    "             for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of multihead attention w/ parallel matrix processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input dimensions\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = d_out // num_heads\n",
    "\n",
    "        # setup attention matrices\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # setup dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n, seq_length, _ = x.shape\n",
    "\n",
    "        # compute Q, K, V matrices\n",
    "        x_query = self.W_q(x)\n",
    "        x_key = self.W_k(x)\n",
    "        x_value = self.W_v(x)\n",
    "\n",
    "        # reshape to separate into Q = [Q1, Q2, ...], K = [K1, K2, ...]\n",
    "        x_query = x_query.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_query = x_query.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        x_key = x_key.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_key = x_key.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "        x_key = x_key.transpose(2, 3) # (n, num_heads, attention_dim, seq_length)\n",
    "\n",
    "        x_value = x_value.view(n, seq_length, self.num_heads, self.attention_dim)\n",
    "        x_value = x_value.transpose(1, 2) # (n, num_heads, seq_length, attention_dim)\n",
    "\n",
    "        # compute attention scores (per-head)\n",
    "        dk_constant = x_key.shape[-1] ** -0.5\n",
    "        mask_context = self.mask.bool()[:seq_length, :seq_length] \n",
    "        attention_scores = (x_query @ x_key)\n",
    "        attention_scores.masked_fill_(mask_context, -torch.inf)\n",
    "\n",
    "        # compute attention weights \n",
    "        # note : no dropout on scores (b/c dropout on -inf is not well-defined)\n",
    "        attention_weights = torch.softmax(attention_scores * dk_constant, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # compute context\n",
    "        context = attention_weights @ x_value\n",
    "\n",
    "        # reshape back to (n, seq_length, d_out)\n",
    "        context = context.contiguous().view(n, seq_length, self.d_out)\n",
    "\n",
    "        # print statements (if necesary)\n",
    "        # print(f\"x_query : {x_query.shape}\")\n",
    "        # print(f\"x_key : {x_key.shape}\")\n",
    "        # print(f\"x_value : {x_value.shape}\")\n",
    "        # print(f\"attention_weights shape: {attention_weights.shape}\")\n",
    "        # print(f\"attention_weights (single head) : {attention_weights[0, 3, :, :]}\")\n",
    "        # print(f\"context : {context.shape}\")\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "# input\n",
    "x = torch.stack([X, X], dim=0)\n",
    "n, seq_length, token_dim = x.shape\n",
    "\n",
    "# hyperparameters\n",
    "x_dim_in = token_dim\n",
    "x_dim_out = 240\n",
    "context_length = seq_length\n",
    "dropout = 0.5\n",
    "\n",
    "# create self attention\n",
    "# single_attention = CasualAttention(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout)\n",
    "# single_attention(x)\n",
    "\n",
    "# multi_attention = MultiHeadAttentionWrapper(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout, num_heads=4)\n",
    "multi_attention = MultiHeadAttention(d_in=x_dim_in, d_out=x_dim_out, context_length=context_length, dropout=dropout, num_heads=4)\n",
    "multi_attention(x)\n",
    "print(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
