{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch\n",
    "This notebook contains code for LLM-from-scratch book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 3 - Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple attention example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- attention --\n",
      "token[1]: tensor([0.5500, 0.8700, 0.6600])\n",
      "A(.) is affinity\n",
      "w(0) = A(x(1), x(0)) : tensor([0.1455])\n",
      "w(1) = A(x(1), x(1)) : tensor([0.2278])\n",
      "w(2) = A(x(1), x(2)) : tensor([0.2249])\n",
      "w(3) = A(x(1), x(3)) : tensor([0.1285])\n",
      "w(4) = A(x(1), x(4)) : tensor([0.1077])\n",
      "w(5) = A(x(1), x(5)) : tensor([0.1656])\n",
      "\n",
      "\n",
      "-- context --\n",
      "list_context_vectors :  torch.Size([6, 3])\n",
      "z(0) = w(0)* x[0] : tensor([0.0625, 0.0218, 0.1295])\n",
      "z(1) = w(1)* x[1] : tensor([0.1253, 0.1982, 0.1504])\n",
      "z(2) = w(2)* x[2] : tensor([0.1282, 0.1911, 0.1439])\n",
      "z(3) = w(3)* x[3] : tensor([0.0283, 0.0745, 0.0424])\n",
      "z(4) = w(4)* x[4] : tensor([0.0830, 0.0269, 0.0108])\n",
      "z(5) = w(5)* x[5] : tensor([0.0083, 0.1325, 0.0911])\n",
      "\n",
      "context_wrt_query:  torch.Size([1, 3])\n",
      "tensor([[0.4355, 0.6451, 0.5680]])\n",
      "\n",
      "\n",
      "-- vectorize --\n",
      "context shape:  torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts (x^3)\n",
    "    [0.22, 0.58, 0.33], # with (x^4)\n",
    "    [0.77, 0.25, 0.10], # one (x^5)\n",
    "    [0.05, 0.80, 0.55] # step (x^6)\n",
    "])\n",
    "\n",
    "# simple affinity : dot-product (to measure similarity)\n",
    "def affinity(x, y):\n",
    "    \"\"\"Given 2 vectors, compute affinity\"\"\"\n",
    "    return torch.dot(x, y)\n",
    "\n",
    "# step 1 : calculate attention weights \n",
    "# idea : If query q : how much should each token of input X (i.e. x1, x2, ...) be weighed in importance \n",
    "# attention(query, x) for all x in input\n",
    "query_idx = 1\n",
    "query_token = X[query_idx]\n",
    "attention_weights = torch.tensor([affinity(x_i, query_token) for (_, x_i) in enumerate(X)])\n",
    "attention_weights = torch.tensor([a / attention_weights.sum() for a in attention_weights])\n",
    "attention_weights = attention_weights.view(-1, 1)\n",
    "\n",
    "print(\"\\n\\n-- attention --\")\n",
    "print(f\"token[{query_idx}]: {query_token}\")\n",
    "print(\"A(.) is affinity\")\n",
    "for idx, score in enumerate(attention_weights):\n",
    "    print(f\"w({idx}) = A(x({query_idx}), x({idx})) : {score}\")\n",
    "\n",
    "# step 2 : compute context vectors  \n",
    "# idea : Given query q and attention weights, create \"information context\" using weighted sum approach\n",
    "# idea : \"information context\" tells LLM how to make use of all the input tokens\n",
    "query = X[1]\n",
    "list_context_vectors = attention_weights * X\n",
    "context_vector = list_context_vectors.sum(dim=0, keepdim=True)\n",
    "print(\"\\n\\n-- context --\")\n",
    "print(\"list_context_vectors : \", list_context_vectors.shape)\n",
    "for idx, vec in enumerate(list_context_vectors):\n",
    "    print(f\"z({idx}) = w({idx})* x[{idx}] : {vec}\")\n",
    "\n",
    "print(\"\\ncontext_wrt_query: \", context_vector.shape)\n",
    "print(context_vector)\n",
    "\n",
    "# step 3 - vectorize \n",
    "print(\"\\n\\n-- vectorize --\")\n",
    "attention_scores = X @ X.T # compute attention pair-wise for each x_i, x_j pair using dot-product \n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) # row_i = attention weights w.r.t x_i\n",
    "context_matrix = attention_weights @ X # output (n, k) where each row i is attention_context for x_i\n",
    "print(\"context shape: \", context_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention \n",
    "Self-attention introduces 3 trainable parameters ($W_q$(query), $W_k$(key), $W_v$(value)) matrices ontop of attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "d_in = X.shape[1]\n",
    "d_out = 2\n",
    "x_2 = X[1]\n",
    "\n",
    "# define trainable parameters\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# step 1 : map X into query(x) and key(x)\n",
    "X_query = X @ W_query \n",
    "X_key = X @ W_key \n",
    "\n",
    "# step 2 : compute attention scores\n",
    "# note : a_ij = query(x_i) dot key(x_j)\n",
    "d_k = X_key.shape[-1]\n",
    "attention_scores = X_query @ X_key.T \n",
    "attention_weights = torch.softmax(attention_scores / d_k **0.5, dim=-1)\n",
    "\n",
    "# step 3 : compute context \n",
    "# idea : context = attention_score * value \n",
    "X_value = X @ W_value\n",
    "context = attention_weights @ X_value\n",
    "\n",
    "# attention_weights.sum(dim=1, keepdim=True)\n",
    "print(X.shape)\n",
    "print(attention_scores.shape)\n",
    "print(attention_weights.shape)\n",
    "print(X_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def update_matrices(self, W_q, W_k, W_v):\n",
    "        self.W_query = nn.Parameter(W_q)\n",
    "        self.W_key = nn.Parameter(W_k)\n",
    "        self.W_value = nn.Parameter(W_v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = x @ self.W_query\n",
    "        x_key = x @ self.W_key\n",
    "        x_value = x @ self.W_value\n",
    "\n",
    "        print(\"x_query:\", x_query)\n",
    "        print(\"x_key:\", x_key)\n",
    "        print(\"x_value:\", x_value)\n",
    "        dk_constant = x_key.shape[-1]\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores / dk_constant **0.5, dim=-1)\n",
    "\n",
    "        # step 3 : compute context \n",
    "        return attention_weights @ x_value\n",
    "\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # step 1 : map X into query(x) and key(x)\n",
    "        x_query = self.W_query(x)\n",
    "        x_key = self.W_key(x)\n",
    "        x_value = self.W_value(x)\n",
    "        dk_constant = x_key.shape[-1]\n",
    "\n",
    "        # step 2 : compute attention\n",
    "        attention_scores = x_query @ x_key.T \n",
    "        attention_weights = torch.softmax(attention_scores / dk_constant **0.5, dim=-1)\n",
    "\n",
    "        # step 3 : compute context \n",
    "        return attention_weights @ x_value\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_query: tensor([[ 0.6600, -0.2047],\n",
      "        [ 0.9091, -0.4471],\n",
      "        [ 0.8960, -0.4419],\n",
      "        [ 0.5034, -0.2633],\n",
      "        [ 0.4088, -0.2232],\n",
      "        [ 0.6628, -0.3292]], grad_fn=<MmBackward0>)\n",
      "x_key: tensor([[ 0.3147, -0.4016],\n",
      "        [-0.0298, -0.4459],\n",
      "        [-0.0170, -0.4262],\n",
      "        [-0.1054, -0.2724],\n",
      "        [ 0.2185,  0.0482],\n",
      "        [-0.2258, -0.4782]], grad_fn=<MmBackward0>)\n",
      "x_value: tensor([[-0.0872,  0.0286],\n",
      "        [-0.1137,  0.0766],\n",
      "        [-0.1018,  0.0927],\n",
      "        [-0.0912, -0.0026],\n",
      "        [ 0.1395,  0.3580],\n",
      "        [-0.2085, -0.1546]], grad_fn=<MmBackward0>)\n",
      "x_query: tensor([[ 0.6600, -0.2047],\n",
      "        [ 0.9091, -0.4471],\n",
      "        [ 0.8960, -0.4419],\n",
      "        [ 0.5034, -0.2633],\n",
      "        [ 0.4088, -0.2232],\n",
      "        [ 0.6628, -0.3292]], grad_fn=<MmBackward0>)\n",
      "x_key: tensor([[ 0.3147, -0.4016],\n",
      "        [-0.0298, -0.4459],\n",
      "        [-0.0170, -0.4262],\n",
      "        [-0.1054, -0.2724],\n",
      "        [ 0.2185,  0.0482],\n",
      "        [-0.2258, -0.4782]], grad_fn=<MmBackward0>)\n",
      "x_value: tensor([[-0.0872,  0.0286],\n",
      "        [-0.1137,  0.0766],\n",
      "        [-0.1018,  0.0927],\n",
      "        [-0.0912, -0.0026],\n",
      "        [ 0.1395,  0.3580],\n",
      "        [-0.2085, -0.1546]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "# exercise 3.1 \n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1.update_matrices(W_k=sa_v2.W_key.weight.T, W_q=sa_v2.W_query.weight.T, W_v=sa_v2.W_value.weight.T)\n",
    "\n",
    "# check same result\n",
    "y_1 = sa_v1.forward(X)\n",
    "y_2 = sa_v2.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4058, -0.4704,  0.2368],\n",
      "        [ 0.2134, -0.2601, -0.5105]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4058, -0.4704,  0.2368],\n",
      "        [ 0.2134, -0.2601, -0.5105]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(sa_v1.W_key)\n",
    "print(sa_v2.W_key.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
